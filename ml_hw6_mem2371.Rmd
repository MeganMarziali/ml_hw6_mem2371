---
title: "Assignment 6"
author: "Megan Marziali"
date: "Feb 17, 2021"
output:
  word_document: default
---

## Assignment Set-Up

```{r message = FALSE}
library(tidyverse)
library(NHANES)
library(Amelia)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(e1071)

data(NHANES)

set.seed(100)
```

## Problem 1: Import and Restrict Data

```{r message = FALSE}
nhanes = NHANES %>% 
  janitor::clean_names() %>% 
  select(
    age, race1, education, hh_income, weight, height, 
    pulse, bmi, phys_active, smoke100, diabetes
  )
```

The NHANES data has 10,000 observations. To investigate missingness, I used the mapping function.

```{r message = FALSE, warning = FALSE}
missmap(nhanes)
```

It seems that education, smoking and pulse have a large amount of missing observations. However, I opted to keep all variables and exclude missing observations.

```{r message = FALSE}
nhanes_restr = nhanes %>% na.omit()
```

With missing observations remove, the total number of observations in this dataset is 6356. I next checked the balance of the outcome observations within the dataset:

```{r message = FALSE}
summary(nhanes_restr$diabetes) %>% 
  knitr::kable()
```

There are 5697 "no" responses, and 659 "yes" responses, for a prevalence of diabetes within this sample of 11.6%. This could be considered a rare outcome, and thus the analysis is conducted assuming rare outcome.

```{r message = FALSE, warning = FALSE}
train.indices = createDataPartition(y = nhanes_restr$diabetes,p = 0.7,list = FALSE)

training = nhanes_restr[train.indices,]
testing = nhanes_restr[-train.indices,]
```

## Problem 2/3/4: Model Fit, Cross-Validation and Accuracy Testing

### Part 1: Classification Tree

The following code chunk runs through model fitting of a classification tree, and selecting appropriate hyperparameters using cross-validation. Final accuracy testing is also accomplished.

```{r}
set.seed(100)

# Creation of the train.control object to be carried through all modeling steps
train.control = trainControl(method = "cv", number = 10, sampling = "down") 
# (Using sampling down method because rare outcome)

# Exploring appropriate hyperparameters via cross-validation 
grid.2 = expand.grid(cp = seq(0.001, 0.3, by = 0.01))
tree.diabetes = train(diabetes~., data = training, method = "rpart",trControl = train.control, tuneGrid = grid.2)
tree.diabetes$bestTune
tree.diabetes

# Exploring hyperparameters with smaller steps
grid.3 = expand.grid(cp = seq(0.0005, 0.02, by = 0.001))
tree.diabetes.2 = train(diabetes~., data = training, method = "rpart",trControl = train.control, tuneGrid = grid.3)
tree.diabetes.2$bestTune
tree.diabetes.2

varImp(tree.diabetes.2)
rpart.plot(tree.diabetes.2$finalModel)

# Using best fit model from above with testing data
pred.diabetes = predict(tree.diabetes.2, testing)
pred.diabetes.prob = predict(tree.diabetes.2, testing, type = "prob")

# Evaluating in testing data with confusion matrix
eval.results = confusionMatrix(pred.diabetes, testing$diabetes, positive = "Yes")
print(eval.results)

# ROC curve
analysis = roc(response = testing$diabetes, predictor = pred.diabetes.prob[,2])
plot(1 - analysis$specificities,
     analysis$sensitivities,
     type = "l",
     ylab = "Sensitiviy",
     xlab = "1-Specificity",
     col = "black",
     lwd = 2,
     main = "Classification Tree ROC Curve for Diabetes") %>% 
  abline(a = 0,b = 1)
```

The calculated accuracy of this model is **0.70**.

### Part 2: Support Vector Classification

```{r}
set.seed(100)

# Exploring appropriate hyperparameters via cross-validation 
svm.diabetes = 
  train(diabetes ~ ., 
        data = training, 
        method = "svmLinear", 
        trControl = train.control, 
        preProcess = c("center", "scale"))
svm.diabetes

# Incorporate different values for cost
svm.diabetes.2 = 
  train(diabetes ~ ., 
        data = training, 
        method = "svmLinear", 
        trControl = train.control,
        preProcess = c("center", "scale"), 
        tuneGrid = expand.grid(C = seq(0.00001,2, length = 30)))
svm.diabetes.2
svm.diabetes.2$finalModel

# Found accuracy to be better for the second model, using this going forward.

# Testing the second SVM model in the testing dataest
svm.pred = predict(svm.diabetes.2, newdata = testing[,1:10])

svm.pred.prob = predict(svm.diabetes.2, testing, type = "raw")

table(svm.pred, testing$diabetes)
confusionMatrix(svm.pred, testing$diabetes, positive = "Yes")
```

The calculated accuracy of this model is **0.86**.

### Part 3: Logistic Regression

```{r}
set.seed(100)

# Building logistic regression model
lr.diabetes = 
  train(diabetes ~ ., 
        data = training, 
        trControl = train.control, 
        method = "glm", 
        family = binomial())

# Running with testing data
lr.pred = predict(lr.diabetes, testing, type = "raw")
confusionMatrix(lr.pred, testing$diabetes, positive = "Yes")
```

The accuracy of this model is **0.72**.

## Problem 5

Given the output above, the best-fitting model is the **support vector classification model**. One limitation of using an SVC model involves limited interpretability. While this model can be used to understand the performance of a single variable, interpretability could be challenging for the reader of the analysis if they are not well-versed in machine learning techniques. An additional limitation of this technique is that SVMs are not suitable for large datasets. Applying this technique for an analysis within a large dataset may result in long computational times when training.


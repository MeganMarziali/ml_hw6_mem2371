---
title: "Assignment 6"
author: "Megan Marziali"
date: "Feb 17, 2021"
output:
  github_document: default
---

## Assignment Set-Up

```{r message = FALSE}
library(tidyverse)
library(NHANES)
library(Amelia)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(e1071)

data(NHANES)

set.seed(100)
```

## Problem 1: Import and Restrict Data

```{r message = FALSE}
nhanes = NHANES %>% 
  janitor::clean_names() %>% 
  select(
    age, race1, education, hh_income, weight, height, 
    pulse, bmi, phys_active, smoke100, diabetes
  )
```

The NHANES data has 10,000 observations. To investigate missingness, I used the mapping function.

```{r message = FALSE, warning = FALSE}
missmap(nhanes)
```

It seems that education, smoking and pulse have a large amount of missing observations. However, I opted to keep all variables and exclude missing observations. Once missing observations were removed, I recoded factor variables to numeric to be able to conduct SVC.

```{r message = FALSE}
nhanes_restr = nhanes %>% na.omit()

nhanes_svm = nhanes_restr %>% 
  mutate(
    race1 = as.numeric(race1),
    education = as.numeric(education),
    hh_income = as.numeric(hh_income),
    phys_active = as.numeric(phys_active),
    smoke100 = as.numeric(smoke100)
  )
```

With missing observations remove, the total number of observations in this dataset is 6356. I next checked the balance of the outcome observations within the dataset:

```{r message = FALSE}
summary(nhanes_restr$diabetes)
```

There are 5697 "no" responses, and 659 "yes" responses, for a prevalence of diabetes within this sample of 11.6%.

```{r message = FALSE, warning = FALSE}
train.indices = createDataPartition(y = nhanes_restr$diabetes,p = 0.7,list = FALSE)

training = nhanes_restr[train.indices,]
testing = nhanes_restr[-train.indices,]
```

## Problem 2

### Problem 2.1: Classification Tree

```{r}
set.seed(100)

train.control = trainControl(method = "cv", number = 10, sampling = "down") #This is how to handle unbalanced outcome data
grid.2 = expand.grid(cp = seq(0.001, 0.3, by = 0.01))
#cp = hyperparameter for classification tree
tree.diabetes = train(diabetes~., data = training, method = "rpart",trControl = train.control, tuneGrid = grid.2)
tree.diabetes$bestTune
tree.diabetes

grid.3 = expand.grid(cp = seq(0.0005, 0.02, by = 0.001))
tree.diabetes = train(diabetes~., data = training, method = "rpart",trControl = train.control, tuneGrid = grid.3)
tree.diabetes$bestTune
tree.diabetes # Calculated accuracy from here

varImp(tree.diabetes)

rpart.plot(tree.diabetes$finalModel)

pred.diabetes = predict(tree.diabetes, testing)
pred.diabetes.prob = predict(tree.diabetes, testing, type = "prob")

eval.results = confusionMatrix(pred.diabetes, testing$diabetes, positive = "Yes")
print(eval.results) # how do we get final accuracy

analysis = roc(response = testing$diabetes, predictor = pred.diabetes.prob[,2])
plot(1 - analysis$specificities,analysis$sensitivities,type = "l",
ylab = "Sensitiviy",xlab = "1-Specificity",col = "black",lwd = 2,
main = "ROC Curve for Greater Firearm Fatalities")
abline(a = 0,b = 1)

# do we need to do an ROC curve, already evaluating with a confusion matrix?

```

The calculated accuracy of this model is 0.74.

### Problem 2.2: Support Vector Classification

```{r}
set.seed(100)
train_control = trainControl(method = "cv", number = 10, sampling = "down")
# do we have to do sampling down again?

svm.diabetes = train(diabetes ~ ., data = training, method = "svmLinear", trControl = train_control, preProcess = c("center", "scale"))
svm.diabetes

#Incorporate different values for cost

svm.diabetes.2 = 
  train(diabetes ~ ., 
        data = training, method = "svmLinear", 
        trControl = train_control,
        preProcess = c("center", "scale"), 
        tuneGrid = expand.grid(C = seq(0.00001,2, length = 30)))
svm.diabetes.2

# why do these have the same accuracy?
svm.diabetes.2$finalModel

svm.pred = predict(svm.diabetes.2, newdata = testing[,1:10])
table(svm.pred, testing$diabetes)
confusionMatrix(svm.pred, testing$diabetes, positive = "Yes")
```

The calculated accuracy of this model is 0.85.

### Problem 2.3: Logistic Regression

```{r}
train.control.3 = trainControl(method = "cv", number = 10, sampling = "down")

lr.diabetes = train(diabetes ~ ., data = training, trControl = train.control.3, method = "glm", family = binomial())

lr.pred = predict(lr.diabetes, testing, type = "raw")
confusionMatrix(lr.pred, testing$diabetes, positive = "Yes")
```

The accuracy of this model is 0.71.
